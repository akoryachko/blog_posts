{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-12 08:46] INFO: Tip amount model logger is initialized!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"tip_amount_model_logger\")\n",
    "logger.setLevel(logging.DEBUG) # lowest level to capture by the logger\n",
    "\n",
    "logger.handlers.clear() # remove existing handlers to not accidentally duplicate them\n",
    "sh = logging.StreamHandler() # handler for printing messages to console. Will need file handler in prod\n",
    "\n",
    "sh.setLevel(logging.INFO) # lowest level for the handler to display\n",
    "f = logging.Formatter(\"[%(asctime)s] %(levelname)s: %(message)s\", \"%Y-%m-%d %H:%M\")\n",
    "sh.setFormatter(f)\n",
    "logger.addHandler(sh)\n",
    "\n",
    "logger.info(\"Tip amount model logger is initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/12 08:46:54 WARN Utils: Your hostname, Alexandrs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.110 instead (on interface en0)\n",
      "26/01/12 08:46:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/12 08:46:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# little trick to make spark work locally\n",
    "findspark.init()\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(dataset_name: str) -> DataFrame:\n",
    "    file_path = f\"../data/{dataset_name}.csv\"\n",
    "    return spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "def extract() -> dict[str, DataFrame]:\n",
    "    logger.info(\"Extracting datasets\")\n",
    "    dataset_names = [\n",
    "        \"taxi_trip_data\",\n",
    "        \"taxi_zone_geo\",\n",
    "    ]\n",
    "    return {dataset_name: read_dataset(dataset_name) for dataset_name in dataset_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_history_to_a_range(sdf: DataFrame) -> DataFrame:\n",
    "    pickup_month = F.date_format(F.col(\"pickup_datetime\"), \"yyyyMM\")\n",
    "    return (\n",
    "        sdf\n",
    "        .filter(pickup_month > history_start_month)\n",
    "        .filter(pickup_month <= history_end_month)\n",
    "    )\n",
    "\n",
    "def keep_evening_rides_only(sdf: DataFrame) -> DataFrame:\n",
    "    dropoff_hour = F.date_format(F.col(\"dropoff_datetime\"), \"HH\")\n",
    "    return (\n",
    "        sdf\n",
    "        .filter(dropoff_hour >= first_evening_hour)\n",
    "        .filter(dropoff_hour <= last_evening_hour)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_airports_by_location(sdf: DataFrame, location_id_col_name: str) -> DataFrame:\n",
    "    sdf_zone_geo_airport = (\n",
    "        sdfs[\"taxi_zone_geo\"]\n",
    "        .filter(F.lower(F.col(\"zone_name\")).like(\"%airport%\"))\n",
    "    )\n",
    "    return (\n",
    "        sdf\n",
    "        .join(\n",
    "            sdf_zone_geo_airport,\n",
    "            on=[F.col(location_id_col_name) == F.col(\"zone_id\")],\n",
    "            how=\"leftanti\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_first_n_daily_rides_only(sdf: DataFrame) -> DataFrame:\n",
    "    pickup_date = F.date_format(F.col(\"pickup_datetime\"), \"yyyyMMdd\")\n",
    "    window = (\n",
    "        Window\n",
    "        .partitionBy(\"pickup_location_id\", pickup_date)\n",
    "        .orderBy(F.asc(\"pickup_datetime\"))\n",
    "    )\n",
    "    return (\n",
    "        sdf\n",
    "        .withColumn(\"ride_number\", F.row_number().over(window))\n",
    "        .filter(F.col(\"ride_number\") <= n_first_daily_rides_to_keep)\n",
    "        .drop(\"ride_number\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(sdf: DataFrame) -> DataFrame:\n",
    "    return (\n",
    "        sdf\n",
    "        .dropDuplicates()\n",
    "        .transform(limit_history_to_a_range)\n",
    "        .transform(keep_evening_rides_only)\n",
    "        .transform(exclude_airports_by_location, \"pickup_location_id\")\n",
    "        .transform(exclude_airports_by_location, \"dropoff_location_id\")\n",
    "        .transform(keep_first_n_daily_rides_only)\n",
    "    )\n",
    "\n",
    "def add_features(sdf: DataFrame) -> DataFrame:\n",
    "    return (\n",
    "        sdf\n",
    "        .withColumn(\"month\", F.month(F.col(\"pickup_datetime\")))\n",
    "        .withColumn(\"day_of_week\", F.dayofweek(F.col(\"pickup_datetime\")))\n",
    "        .withColumn(\"day_of_month\", F.dayofmonth(F.col(\"pickup_datetime\")))\n",
    "        .withColumn(\"store_and_fwd_flag\", F.when(F.col(\"store_and_fwd_flag\") == \"N\", 0).otherwise(1))\n",
    "    )\n",
    "\n",
    "def transform() -> PipelineModel:\n",
    "    logger.info(\"Preparing the data\")\n",
    "    sdfs[\"prepared_data\"] = (\n",
    "        sdfs[\"taxi_trip_data\"]\n",
    "        .transform(filter_data)\n",
    "        .transform(add_features)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split() -> tuple[DataFrame]:\n",
    "    return (\n",
    "        sdfs[\"prepared_data\"]\n",
    "        .randomSplit(\n",
    "            weights=[1-test_fraction, test_fraction],\n",
    "            seed=42\n",
    "        )\n",
    "    )\n",
    "\n",
    "def train_model() -> PipelineModel:\n",
    "\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=feature_cols,\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "\n",
    "    rf = RandomForestRegressor(\n",
    "        labelCol=\"tip_amount\",\n",
    "        featuresCol=\"features\",\n",
    "        predictionCol=\"prediction\",\n",
    "        numTrees=10,\n",
    "        maxDepth=4,\n",
    "        featureSubsetStrategy=\"auto\",\n",
    "        seed=42,\n",
    "        bootstrap=True,\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "    return pipeline.fit(sdfs[\"training\"])\n",
    "\n",
    "def train() -> PipelineModel:\n",
    "    logger.info(\"Start training\")\n",
    "\n",
    "    sdfs[\"training\"], sdfs[\"test\"] = train_test_split()\n",
    "\n",
    "    logger.info(\"Training the model\")\n",
    "    model = train_model()\n",
    "    logger.info(\"Model is trained\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_features_importances() -> None:\n",
    "    logger.info(\"  Features importances\")\n",
    "    importances = zip(feature_cols, model.stages[-1].featureImportances, strict=False)\n",
    "    for name, importance in sorted(importances, key=lambda item: item[1], reverse=True):\n",
    "        logger.info(\"%22s = %.2g\", name, importance)\n",
    "\n",
    "def evaluate_on_dataset(sdf: DataFrame) -> None:\n",
    "    evaluator = RegressionEvaluator()\n",
    "    evaluator.setPredictionCol(\"prediction\")\n",
    "    evaluator.setLabelCol(\"tip_amount\")\n",
    "\n",
    "    evaluation_metrics = [\"rmse\", \"mae\", \"r2\"]\n",
    "\n",
    "    sdf_predictions = model.transform(sdf)\n",
    "\n",
    "    for metric_name in evaluation_metrics:\n",
    "        value = evaluator.evaluate(sdf_predictions, {evaluator.metricName: metric_name})\n",
    "        logger.info(\"%8s = %.2g\", metric_name, value)\n",
    "\n",
    "def check_evaluation_metrics() -> None:\n",
    "    for set_name in [\"training\", \"test\"]:\n",
    "        logger.info(\"  Evaluation on the %s set\", set_name)\n",
    "        evaluate_on_dataset(sdfs[set_name])\n",
    "\n",
    "def validate() -> None:\n",
    "    logger.info(\"Start validation\")\n",
    "    check_features_importances()\n",
    "    check_evaluation_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load() -> None:\n",
    "    logger.info(\"Saving the model\")\n",
    "    model.write().overwrite().save(\"../data/model\")\n",
    "    logger.info(\"The model is saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_start_month = \"201703\"\n",
    "history_end_month = \"201811\"\n",
    "first_evening_hour = \"17\"\n",
    "last_evening_hour = \"23\"\n",
    "n_first_daily_rides_to_keep = 3\n",
    "test_fraction = 0.2\n",
    "feature_cols = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"rate_code\",\n",
    "    \"store_and_fwd_flag\",\n",
    "    \"payment_type\",\n",
    "    \"fare_amount\",\n",
    "    \"tolls_amount\",\n",
    "    \"imp_surcharge\",\n",
    "    \"month\",\n",
    "    \"day_of_week\",\n",
    "    \"day_of_month\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-12 08:47] INFO: Extracting datasets\n",
      "[2026-01-12 08:47] INFO: Preparing the data                                     \n",
      "[2026-01-12 08:47] INFO: Start training\n",
      "[2026-01-12 08:47] INFO: Training the model\n",
      "[2026-01-12 08:47] INFO: Model is trained                                       \n",
      "[2026-01-12 08:47] INFO: Start validation\n",
      "[2026-01-12 08:47] INFO:   Features importances\n",
      "[2026-01-12 08:47] INFO:           payment_type = 0.65\n",
      "[2026-01-12 08:47] INFO:            fare_amount = 0.22\n",
      "[2026-01-12 08:47] INFO:          trip_distance = 0.052\n",
      "[2026-01-12 08:47] INFO:              rate_code = 0.049\n",
      "[2026-01-12 08:47] INFO:           tolls_amount = 0.028\n",
      "[2026-01-12 08:47] INFO:           day_of_month = 0.0023\n",
      "[2026-01-12 08:47] INFO:                  month = 0.0019\n",
      "[2026-01-12 08:47] INFO:            day_of_week = 0.0017\n",
      "[2026-01-12 08:47] INFO:          imp_surcharge = 0.00076\n",
      "[2026-01-12 08:47] INFO:        passenger_count = 0.0007\n",
      "[2026-01-12 08:47] INFO:     store_and_fwd_flag = 5.1e-05\n",
      "[2026-01-12 08:47] INFO:   Evaluation on the training set\n",
      "[2026-01-12 08:47] INFO:     rmse = 3.1                                         \n",
      "[2026-01-12 08:47] INFO:      mae = 1.6                                         \n",
      "[2026-01-12 08:47] INFO:       r2 = 0.36\n",
      "[2026-01-12 08:47] INFO:   Evaluation on the test set\n",
      "[2026-01-12 08:47] INFO:     rmse = 3.2\n",
      "[2026-01-12 08:47] INFO:      mae = 1.7                                         \n",
      "[2026-01-12 08:47] INFO:       r2 = 0.35\n",
      "[2026-01-12 08:47] INFO: Saving the model\n",
      "[2026-01-12 08:47] INFO: The model is saved\n"
     ]
    }
   ],
   "source": [
    "sdfs = extract()\n",
    "transform()\n",
    "model = train()\n",
    "validate()\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_to_production_blogpost",
   "language": "python",
   "name": "pyspark_to_production_blogpost"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
