{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# little trick to make spark work locally\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/12 16:10:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config(\"spark.driver.bindAddress\", \"127.0.0.1\").config(\"spark.driver.host\", \"127.0.0.1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure changes in imported modules become available without a need to restart\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pointing spark to search the folder with our modules for imports\n",
    "import sys\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-12 16:12] INFO: pyspark_to_production.src.tip_amount_model logger is initialized!\n"
     ]
    }
   ],
   "source": [
    "from pyspark_to_production.src.tip_amount_model import TipAmountModelConfig, TipAmountModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TipAmountModelConfig()\n",
    "job = TipAmountModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def is_subset(a: list, b: list) -> bool:\n",
    "    return set(a) <= set(b)\n",
    "\n",
    "data = [\n",
    "    (datetime(2021, 1, 1, 12, 0, 0), \"Y\"),\n",
    "    (datetime(2021, 6, 15, 9, 30, 0), \"N\")\n",
    "]\n",
    "\n",
    "expected_columns = job.feature_cols[-4:]\n",
    "\n",
    "sdf_fake_input = job.spark.createDataFrame(data, schema=[\"pickup_datetime\", \"store_and_fwd_flag\"])\n",
    "assert not is_subset(expected_columns, sdf_fake_input.columns)\n",
    "\n",
    "sdf_fake_features = job.add_features(sdf_fake_input)\n",
    "assert is_subset(expected_columns, sdf_fake_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "from typing import TypeVar, Type\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "@dataclass\n",
    "class Trip:\n",
    "    vendor_id: int = 1\n",
    "    pickup_datetime: datetime = datetime(2018, 2, 4, 18, 0, 0)\n",
    "    dropoff_datetime: datetime = datetime(2018, 2, 4, 19, 30, 0)\n",
    "    passenger_count: int = 2\n",
    "    trip_distance: float = 50.2\n",
    "    rate_code: int = 3\n",
    "    store_and_fwd_flag: str = \"N\"\n",
    "    payment_type: int = 1\n",
    "    fare_amount: float = 10.5\n",
    "    extra: float = 0.1\n",
    "    mta_tax: float = 0.5\n",
    "    tip_amount: float = 0.8\n",
    "    tolls_amount: float = 0.1\n",
    "    imp_surcharge: float = 1.2\n",
    "    total_amount: float = 15.2\n",
    "    pickup_location_id: int = 1\n",
    "    dropoff_location_id: int = 2\n",
    "\n",
    "@dataclass\n",
    "class ZoneGeo:\n",
    "    zone_id: int = 1\n",
    "    zone_name: str = \"Snack Zone\"\n",
    "    borough: str = \"Food Borough\"\n",
    "\n",
    "def generate_rows(data_class: Type[T], data: list[tuple] = [()], columns: list[str] = []) -> list[Row]:\n",
    "    generated_rows = []\n",
    "    for record in data:\n",
    "        record_dict = dict(zip(columns, record))\n",
    "        record_class = data_class(**record_dict)\n",
    "        record_row = Row(**asdict(record_class))\n",
    "        generated_rows.append(record_row)\n",
    "    return generated_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-12 16:12] INFO: Preparing the data for training\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def is_subset(a: list, b: list) -> bool:\n",
    "    return set(a) <= set(b)\n",
    "\n",
    "\n",
    "def test_add_features_column_names() -> None:\n",
    "    columns=[\"pickup_datetime\", \"store_and_fwd_flag\"]\n",
    "    data = [\n",
    "        (datetime(2021, 1, 1, 12, 0, 0), \"Y\"),\n",
    "        (datetime(2021, 6, 15, 9, 30, 0), \"N\")\n",
    "    ]\n",
    "\n",
    "    tip_model = TipAmountModel(TipAmountModelConfig())\n",
    "\n",
    "    tip_model.sdfs[\"taxi_trip_data\"] = tip_model.spark.createDataFrame(generate_rows(Trip, data, columns))\n",
    "    tip_model.sdfs[\"taxi_zone_geo\"] = tip_model.spark.createDataFrame(generate_rows(ZoneGeo))\n",
    "\n",
    "    assert not is_subset(tip_model.feature_cols, tip_model.sdfs[\"taxi_trip_data\"].columns)\n",
    "\n",
    "    tip_model.transform()\n",
    "    assert is_subset(tip_model.feature_cols, tip_model.sdfs[\"prepared_data\"].columns)\n",
    "\n",
    "\n",
    "test_add_features_column_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-12 16:13] INFO: Preparing the data for training\n",
      "[2026-01-12 16:13] INFO: Preparing the data for training                        \n"
     ]
    }
   ],
   "source": [
    "def test_exclude_airports_by_location() -> None:\n",
    "    columns=[\"pickup_location_id\", \"dropoff_location_id\"]\n",
    "    data = [\n",
    "        (1, 1),\n",
    "        (100, 1),\n",
    "        (1, 100),\n",
    "        (100, 100),\n",
    "    ]\n",
    "\n",
    "    tip_model = TipAmountModel(TipAmountModelConfig())\n",
    "\n",
    "    # no airports\n",
    "    tip_model.sdfs[\"taxi_trip_data\"] = tip_model.spark.createDataFrame(generate_rows(Trip, data, columns))\n",
    "    tip_model.sdfs[\"taxi_zone_geo\"] = tip_model.spark.createDataFrame(\n",
    "        generate_rows(ZoneGeo, [(100, \"terrestrial\", ), [\"zone_id\", \"zone_name\"]])\n",
    "    )\n",
    "\n",
    "    tip_model.transform()\n",
    "    assert tip_model.sdfs[\"prepared_data\"].count() == 4\n",
    "\n",
    "    # all except one have airports\n",
    "    tip_model.sdfs[\"taxi_zone_geo\"] = tip_model.spark.createDataFrame(\n",
    "        generate_rows(ZoneGeo, [(100, \"is airport or so\", )], [\"zone_id\", \"zone_name\"])\n",
    "    )\n",
    "\n",
    "    tip_model.transform()\n",
    "    assert tip_model.sdfs[\"prepared_data\"].count() == 1\n",
    "\n",
    "test_exclude_airports_by_location()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_to_production_blogpost",
   "language": "python",
   "name": "pyspark_to_production_blogpost"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
