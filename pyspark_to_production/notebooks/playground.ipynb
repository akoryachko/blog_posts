{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# little trick to make spark work locally\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure changes in imported modules become available without a need to restart\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pointing spark to search the folder with our modules for imports\n",
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-09 10:32] INFO: tip_amount_model logger is initialized!\n"
     ]
    }
   ],
   "source": [
    "from tip_amount_model import TipAmountModelConfig, TipAmountModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/09 10:32:19 WARN Utils: Your hostname, Alexandrs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.108 instead (on interface en0)\n",
      "26/01/09 10:32:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/09 10:32:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "config = TipAmountModelConfig()\n",
    "job = TipAmountModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-09 10:32] INFO: Extracting datasets\n",
      "[2026-01-09 10:32] INFO: Preparing the data for training                        \n",
      "[2026-01-09 10:32] INFO: Training the model\n",
      "[2026-01-09 10:32] INFO: Start validation                                       \n",
      "[2026-01-09 10:32] INFO:   Features importances\n",
      "[2026-01-09 10:32] INFO:           payment_type = 0.7\n",
      "[2026-01-09 10:32] INFO:            fare_amount = 0.17\n",
      "[2026-01-09 10:32] INFO:          trip_distance = 0.053\n",
      "[2026-01-09 10:32] INFO:           tolls_amount = 0.037\n",
      "[2026-01-09 10:32] INFO:              rate_code = 0.032\n",
      "[2026-01-09 10:32] INFO:          imp_surcharge = 0.0039\n",
      "[2026-01-09 10:32] INFO:           day_of_month = 0.0032\n",
      "[2026-01-09 10:32] INFO:        passenger_count = 0.0031\n",
      "[2026-01-09 10:32] INFO:                  month = 0.00049\n",
      "[2026-01-09 10:32] INFO:            day_of_week = 0.00024\n",
      "[2026-01-09 10:32] INFO: store_and_fwd_flag_is_N = 9.3e-06\n",
      "[2026-01-09 10:32] INFO:   Evaluation on the training set\n",
      "[2026-01-09 10:32] INFO:     rmse = 3.1                                         \n",
      "[2026-01-09 10:32] INFO:      mae = 1.6\n",
      "[2026-01-09 10:32] INFO:       r2 = 0.36\n",
      "[2026-01-09 10:32] INFO:   Evaluation on the test set\n",
      "[2026-01-09 10:32] INFO:     rmse = 3.2                                         \n",
      "[2026-01-09 10:32] INFO:      mae = 1.7\n",
      "[2026-01-09 10:32] INFO:       r2 = 0.35\n",
      "[2026-01-09 10:32] INFO: Saving the model\n",
      "[2026-01-09 10:32] INFO: The model is saved\n"
     ]
    }
   ],
   "source": [
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['passenger_count',\n",
       " 'trip_distance',\n",
       " 'rate_code',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'tolls_amount',\n",
       " 'imp_surcharge',\n",
       " 'month',\n",
       " 'day_of_week',\n",
       " 'day_of_month',\n",
       " 'store_and_fwd_flag_is_N']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 92:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+---------------+-------------+---------+------------------+------------+-----------+-----+-------+----------+------------+-------------+------------+------------------+-------------------+-----+-----------+------------+-----------------------+\n",
      "|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|rate_code|store_and_fwd_flag|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|imp_surcharge|total_amount|pickup_location_id|dropoff_location_id|month|day_of_week|day_of_month|store_and_fwd_flag_is_N|\n",
      "+---------+-------------------+-------------------+---------------+-------------+---------+------------------+------------+-----------+-----+-------+----------+------------+-------------+------------+------------------+-------------------+-----+-----------+------------+-----------------------+\n",
      "|        1|2018-01-01 16:48:36|2018-01-01 17:20:58|              4|         10.0|        1|                 N|           1|       32.5|  0.0|    0.5|       4.0|         0.0|          0.3|        37.3|               231|                116|    1|          2|           1|                      1|\n",
      "|        1|2018-01-01 17:30:05|2018-01-01 18:02:16|              2|          7.5|        1|                 N|           1|       26.5|  0.0|    0.5|      5.45|         0.0|          0.3|       32.75|                48|                 33|    1|          2|           1|                      1|\n",
      "|        1|2018-01-01 18:43:01|2018-01-01 19:00:15|              1|         10.9|        1|                 N|           1|       30.5|  0.0|    0.5|      3.06|        2.64|          0.3|        37.0|                48|                200|    1|          2|           1|                      1|\n",
      "|        1|2018-01-01 19:21:52|2018-01-01 19:41:51|              1|          7.6|        1|                 N|           1|       23.5|  0.0|    0.5|      7.25|         0.0|          0.3|       31.55|               231|                 75|    1|          2|           1|                      1|\n",
      "|        1|2018-01-01 19:39:25|2018-01-01 19:59:28|              1|          6.5|        1|                 N|           1|       22.0|  0.0|    0.5|      4.55|         0.0|          0.3|       27.35|               209|                255|    1|          2|           1|                      1|\n",
      "+---------+-------------------+-------------------+---------------+-------------+---------+------------------+------------+-----------+-----+-------+----------+------------+-------------+------------+------------------+-------------------+-----+-----------+------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "job.sdfs[\"training\"].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out different parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-09 10:32] INFO: Preparing the data for training\n",
      "[2026-01-09 10:32] INFO: Training the model\n",
      "[2026-01-09 10:32] INFO: Start validation\n",
      "[2026-01-09 10:32] INFO:   Features importances\n",
      "[2026-01-09 10:32] INFO:           payment_type = 0.72\n",
      "[2026-01-09 10:32] INFO:            fare_amount = 0.17\n",
      "[2026-01-09 10:32] INFO:          trip_distance = 0.045\n",
      "[2026-01-09 10:32] INFO:           tolls_amount = 0.03\n",
      "[2026-01-09 10:32] INFO:              rate_code = 0.029\n",
      "[2026-01-09 10:32] INFO:        passenger_count = 0.0036\n",
      "[2026-01-09 10:32] INFO:          imp_surcharge = 0.0018\n",
      "[2026-01-09 10:32] INFO:            day_of_week = 0.0012\n",
      "[2026-01-09 10:32] INFO:                  month = 0.00072\n",
      "[2026-01-09 10:32] INFO:           day_of_month = 0.00058\n",
      "[2026-01-09 10:32] INFO: store_and_fwd_flag_is_N = 0\n",
      "[2026-01-09 10:32] INFO:   Evaluation on the training set\n",
      "[2026-01-09 10:32] INFO:     rmse = 3.1\n",
      "[2026-01-09 10:32] INFO:      mae = 1.6\n",
      "[2026-01-09 10:32] INFO:       r2 = 0.36\n",
      "[2026-01-09 10:32] INFO:   Evaluation on the test set\n",
      "[2026-01-09 10:32] INFO:     rmse = 2.9\n",
      "[2026-01-09 10:32] INFO:      mae = 1.6\n",
      "[2026-01-09 10:32] INFO:       r2 = 0.41\n"
     ]
    }
   ],
   "source": [
    "job.config.test_fraction = 0.01\n",
    "job.transform()\n",
    "job.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "def train_model(self) -> None:\n",
    "    assembler = VectorAssembler(inputCols=self.feature_cols, outputCol=\"features\")\n",
    "\n",
    "    gbt = GBTRegressor(\n",
    "        labelCol=\"tip_amount\",\n",
    "        featuresCol=\"features\",\n",
    "        predictionCol=\"prediction\",\n",
    "        stepSize=0.1,\n",
    "        maxDepth=4,\n",
    "        featureSubsetStrategy=\"auto\",\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(stages=[assembler, gbt])\n",
    "\n",
    "    self.model = pipeline.fit(self.sdfs[\"training\"])\n",
    "\n",
    "    print(\"Modified content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "job.train_model = types.MethodType(train_model, job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-09 10:32] INFO: Preparing the data for training\n",
      "[2026-01-09 10:32] INFO: Training the model\n",
      "[2026-01-09 10:32] INFO: Start validation\n",
      "[2026-01-09 10:32] INFO:   Features importances\n",
      "[2026-01-09 10:32] INFO:           payment_type = 0.37\n",
      "[2026-01-09 10:32] INFO:            fare_amount = 0.22\n",
      "[2026-01-09 10:32] INFO:           tolls_amount = 0.076\n",
      "[2026-01-09 10:32] INFO:          trip_distance = 0.069\n",
      "[2026-01-09 10:32] INFO:        passenger_count = 0.059\n",
      "[2026-01-09 10:32] INFO:           day_of_month = 0.059\n",
      "[2026-01-09 10:32] INFO:                  month = 0.05\n",
      "[2026-01-09 10:32] INFO:              rate_code = 0.047\n",
      "[2026-01-09 10:32] INFO:          imp_surcharge = 0.026\n",
      "[2026-01-09 10:32] INFO:            day_of_week = 0.02\n",
      "[2026-01-09 10:32] INFO: store_and_fwd_flag_is_N = 0\n",
      "[2026-01-09 10:32] INFO:   Evaluation on the training set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/09 10:32:52 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "26/01/09 10:32:52 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "[2026-01-09 10:32] INFO:     rmse = 3\n",
      "[2026-01-09 10:32] INFO:      mae = 1.4\n",
      "[2026-01-09 10:32] INFO:       r2 = 0.42\n",
      "[2026-01-09 10:32] INFO:   Evaluation on the test set\n",
      "[2026-01-09 10:32] INFO:     rmse = 2.8                                         \n",
      "[2026-01-09 10:32] INFO:      mae = 1.4\n",
      "[2026-01-09 10:32] INFO:       r2 = 0.48                                        \n"
     ]
    }
   ],
   "source": [
    "job.transform()\n",
    "job.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def is_subset(a: list, b: list) -> bool:\n",
    "    return set(a) <= set(b)\n",
    "\n",
    "data = [\n",
    "    (datetime(2021, 1, 1, 12, 0, 0), \"Y\"),\n",
    "    (datetime(2021, 6, 15, 9, 30, 0), \"N\")\n",
    "]\n",
    "\n",
    "expected_columns = job.feature_cols[-4:]\n",
    "\n",
    "sdf_fake_input = job.spark.createDataFrame(data, schema=[\"pickup_datetime\", \"store_and_fwd_flag\"])\n",
    "assert not is_subset(expected_columns, sdf_fake_input.columns)\n",
    "\n",
    "sdf_fake_features = job.add_features(sdf_fake_input)\n",
    "assert is_subset(expected_columns, sdf_fake_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['month', 'day_of_week', 'day_of_month', 'store_and_fwd_flag_is_N']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.feature_cols[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pickup_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'month',\n",
       " 'day_of_week',\n",
       " 'day_of_month',\n",
       " 'store_and_fwd_flag_is_N']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_fake_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----+-----------+------------+-----------------------+\n",
      "|    pickup_datetime|store_and_fwd_flag|month|day_of_week|day_of_month|store_and_fwd_flag_is_N|\n",
      "+-------------------+------------------+-----+-----------+------------+-----------------------+\n",
      "|2021-01-01 12:00:00|                 Y|    1|          6|           1|                      0|\n",
      "|2021-06-15 09:30:00|                 N|    6|          3|          15|                      1|\n",
      "|2022-03-03 23:59:59|              NULL|    3|          5|           3|                      0|\n",
      "+-------------------+------------------+-----+-----------+------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_fake_features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "from typing import TypeVar, Type\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "@dataclass\n",
    "class Trip:\n",
    "    vendor_id: int = 1\n",
    "    pickup_datetime: datetime = datetime(2018, 2, 4, 11, 0, 0)\n",
    "    dropoff_datetime: datetime = datetime(2018, 2, 4, 12, 30, 0)\n",
    "    passenger_count: int = 2\n",
    "    trip_distance: float = 50.2\n",
    "    rate_code: int = 3\n",
    "    store_and_fwd_flag: str = \"N\"\n",
    "    payment_type: int = 1\n",
    "    fare_amount: float = 10.5\n",
    "    extra: float = 0.1\n",
    "    mta_tax: float = 0.5\n",
    "    tip_amount: float = 0.8\n",
    "    tolls_amount: float = 0.1\n",
    "    imp_surcharge: float = 1.2\n",
    "    total_amount: float = 15.2\n",
    "    pickup_location_id: int = 1\n",
    "    dropoff_location_id: int = 2\n",
    "\n",
    "@dataclass\n",
    "class ZoneGeo:\n",
    "    zone_id: int = 1\n",
    "    zone_name: str = \"Snack Zone\"\n",
    "    borough: str = \"Food Borough\"\n",
    "\n",
    "def generate_rows(data_class: Type[T], data: list[tuple] = [()], columns: list[str] = []) -> list[Row]:\n",
    "    generated_rows = []\n",
    "    for record in data:\n",
    "        record_dict = dict(zip(columns, record))\n",
    "        record_class = data_class(**record_dict)\n",
    "        record_row = Row(**asdict(record_class))\n",
    "        generated_rows.append(record_row)\n",
    "    return generated_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(zone_id=1, zone_name='Snack Zone', borough='Food Borough')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_rows(ZoneGeo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZoneGeo(zone_id='5', zone_name='Snack Zone', borough='Food Borough')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ZoneGeo(**{\"zone_id\":\"5\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def is_subset(a: list, b: list) -> bool:\n",
    "    return set(a) <= set(b)\n",
    "\n",
    "\n",
    "def test_add_features_columns():\n",
    "    columns=[\"pickup_datetime\", \"store_and_fwd_flag\"]\n",
    "    data = [\n",
    "        (datetime(2021, 1, 1, 12, 0, 0), \"Y\"),\n",
    "        (datetime(2021, 6, 15, 9, 30, 0), \"N\")\n",
    "    ]\n",
    "\n",
    "    tip_model = TipAmountModel(TipAmountModelConfig())\n",
    "\n",
    "    tip_model.sdfs[\"taxi_trip_data\"] = tip_model.spark.createDataFrame(generate_rows(Trip, data, columns))\n",
    "    tip_model.sdfs[\"taxi_zone_geo\"] = tip_model.spark.createDataFrame(generate_rows(ZoneGeo))\n",
    "\n",
    "    assert not is_subset(tip_model.feature_cols, tip_model.sdfs[\"taxi_trip_data\"].columns)\n",
    "\n",
    "    tip_model.prepare_data()\n",
    "    assert is_subset(tip_model.feature_cols, tip_model.sdfs[\"prepared_data\"].columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_to_production_blogpost",
   "language": "python",
   "name": "pyspark_to_production_blogpost"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
