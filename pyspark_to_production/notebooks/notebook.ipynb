{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# little trick to make spark work locally\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/08 08:55:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/01/08 08:55:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"tip_amount_model\").config(\"spark.driver.bindAddress\", \"127.0.0.1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "sdf_taxi_data = spark.read.csv(\"../data/taxi_trip_data.csv\", header=True, inferSchema=True)\n",
    "sdf_taxi_zone_geo = spark.read.csv(\"../data/taxi_zone_geo.csv\", header=True, inferSchema=True)\n",
    "\n",
    "sdf_prepared_data = (\n",
    "    sdf_taxi_data\n",
    "    .dropDuplicates()\n",
    "    # keep only the evening rides in a time range\n",
    "    .filter(F.date_format(F.col(\"pickup_datetime\"), \"yyyyMM\") > \"201703\")\n",
    "    .filter(F.date_format(F.col(\"pickup_datetime\"), \"yyyyMM\") <= \"201811\")\n",
    "    .filter(F.date_format(F.col(\"dropoff_datetime\"), \"HH\") >= \"17\")\n",
    "    .filter(F.date_format(F.col(\"dropoff_datetime\"), \"HH\") <= \"23\")\n",
    "    # remove rides to and from the airport\n",
    "    .join(\n",
    "        sdf_taxi_zone_geo\n",
    "        .withColumnRenamed(\"zone_id\", \"pickup_location_id\"),\n",
    "        on=\"pickup_location_id\"\n",
    "    )\n",
    "    .withColumnRenamed(\"zone_name\", \"pickup_zone_name\")\n",
    "    .withColumnRenamed(\"borough\", \"pickup_borough\")\n",
    "    .filter(~F.lower(F.col(\"pickup_zone_name\")).like(\"%airport%\"))\n",
    "    .join(\n",
    "        sdf_taxi_zone_geo\n",
    "        .withColumnRenamed(\"zone_id\", \"dropoff_location_id\"),\n",
    "        on=\"dropoff_location_id\"\n",
    "    )\n",
    "    .withColumnRenamed(\"zone_name\", \"dropoff_zone_name\")\n",
    "    .withColumnRenamed(\"borough\", \"dropoff_borough\")\n",
    "    .filter(~F.lower(F.col(\"dropoff_zone_name\")).like(\"%airport%\"))\n",
    "    # take the first 3 rides per day for each pickup location\n",
    "    .withColumn(\n",
    "        \"row_number\",\n",
    "        F.row_number()\n",
    "        .over(\n",
    "            Window\n",
    "            .partitionBy(\"pickup_location_id\", F.date_format(F.col(\"pickup_datetime\"), \"ddMMyyyy\"))\n",
    "            .orderBy(F.asc(\"pickup_datetime\"))\n",
    "        )\n",
    "    )\n",
    "    .filter(F.col(\"row_number\") < 4)\n",
    "    # features engineering\n",
    "    .withColumn(\"month\", F.month(F.col(\"pickup_datetime\")))\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(F.col(\"pickup_datetime\")))\n",
    "    .withColumn(\"day_of_month\", F.dayofmonth(F.col(\"pickup_datetime\")))\n",
    "    .withColumn(\"store_and_fwd_flag\", F.when(F.col(\"store_and_fwd_flag\") == \"N\", 0).otherwise(1))\n",
    "    .withColumn(\"random_number\", F.rand())\n",
    ")\n",
    "\n",
    "# train test split\n",
    "sdf_training = sdf_prepared_data.filter(F.col(\"random_number\") > 0.2)\n",
    "sdf_test = sdf_prepared_data.filter(F.col(\"random_number\") <= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "feature_cols = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"rate_code\",\n",
    "    \"store_and_fwd_flag\",\n",
    "    \"payment_type\",\n",
    "    \"fare_amount\",\n",
    "    \"tolls_amount\",\n",
    "    \"imp_surcharge\",\n",
    "    \"month\",\n",
    "    \"day_of_week\",\n",
    "    \"day_of_month\",\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    labelCol=\"tip_amount\",\n",
    "    featuresCol=\"features\",\n",
    "    predictionCol=\"prediction\",\n",
    "    numTrees=10,\n",
    "    maxDepth=4,\n",
    "    featureSubsetStrategy=\"auto\",\n",
    "    seed=42,\n",
    "    bootstrap=True,\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "model = pipeline.fit(sdf_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passenger_count': np.float64(0.0005283424469808088),\n",
       " 'trip_distance': np.float64(0.04446343766282397),\n",
       " 'rate_code': np.float64(0.05088027237885443),\n",
       " 'store_and_fwd_flag': np.float64(0.0),\n",
       " 'payment_type': np.float64(0.6481006882797419),\n",
       " 'fare_amount': np.float64(0.22206275782788149),\n",
       " 'tolls_amount': np.float64(0.029572308694520134),\n",
       " 'imp_surcharge': np.float64(0.0006402185642648316),\n",
       " 'month': np.float64(0.0022455171205088627),\n",
       " 'day_of_week': np.float64(1.5729102149322334e-05),\n",
       " 'day_of_month': np.float64(0.001490727922274248)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = model.stages[-1].featureImportances\n",
    "importance_dict = dict(zip(feature_cols, importances, strict=False))\n",
    "importance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegressionEvaluator_b03aee9178ae"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator()\n",
    "evaluator.setPredictionCol(\"prediction\")\n",
    "evaluator.setLabelCol(\"tip_amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.122559857683818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35981139615648905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6291385557044495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf_train_predictions = model.transform(sdf_training)\n",
    "print(evaluator.evaluate(sdf_train_predictions, {evaluator.metricName: \"rmse\"}))\n",
    "print(evaluator.evaluate(sdf_train_predictions, {evaluator.metricName: \"r2\"}))\n",
    "print(evaluator.evaluate(sdf_train_predictions, {evaluator.metricName: \"mae\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1718932814766116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35899696919640944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6342744075269313\n"
     ]
    }
   ],
   "source": [
    "sdf_test_predictions = model.transform(sdf_test)\n",
    "print(evaluator.evaluate(sdf_test_predictions, {evaluator.metricName: \"rmse\"}))\n",
    "print(evaluator.evaluate(sdf_test_predictions, {evaluator.metricName: \"r2\"}))\n",
    "print(evaluator.evaluate(sdf_test_predictions, {evaluator.metricName: \"mae\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(\"../data/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_to_production_blogpost",
   "language": "python",
   "name": "pyspark_to_production_blogpost"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
