{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Spark\n",
    "### Big data processing\n",
    "<img src=\"pics/global-data-generated-annually.webp\" alt=\"The rate of data accumulation\" width=\"800\"/>\n",
    "\n",
    "We will produce more data next year than all data produced prior to 2020.\n",
    "\n",
    "At some point the data that needs processing exceeds the size of RAM on a single machine.\n",
    "\n",
    "At this stage most of the in-memory processing tools including Pandas would raise an `OutOfMemory` exception.\n",
    "\n",
    "Some packages can swap data between RAM and the permanent storage but it slows down the workflow a lot.\n",
    "\n",
    "In addition, most of the queries can be parallelized but a single machine can not go beyond the number of its cores even if multithreading.\n",
    "\n",
    "One solution would be to beef up the data processing server but it makes the price raise faster than the specs and up to a certain limit.\n",
    "\n",
    "Another solution is to connect a number of cheap machines together and distribute the data processing tasks among them.\n",
    "\n",
    "This turned out to scale up pretty well and gave a rise to Spark as a system that distributed the computational load across the cluster of devices and collects the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When Spark\n",
    "\n",
    "1. A lot of rows (>8M). Spark can process Terabytes of data, and with some effort can scale up to Petabytes (https://www.databricks.com/blog/2014/10/10/spark-petabyte-sort.html). It does however also mean that Spark is not very well suited for small data (up to a few million rows), it will work, but the overhead will be large. Pandas/Polars/Python would be better suited for analysis on small datasets.\n",
    "2. Column-wise processing. Better database solutions exist for fast retrieval and saving of individual rows. Spark shines when column-wise operations like filtering, aggregation, transaformation, and so on are required\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Spark\n",
    "\n",
    "## Distributed computing framework\n",
    "Distributed computing is the method of making multiple computers work together to solve a common problem.\n",
    "\n",
    "For example, we need to perform some operations on a 100 GB file. Processing this data on a single machine can take hours or maybe days based on the operation. \n",
    "However, if the same file could be broken down into 100 files of 1GB each and then processed in parallel, our total time taken would become approximately 1/100th.\n",
    "\n",
    "A Spark cluster consists of the following components shown in the image below:\n",
    "<img src=\"pics/cluster-overview.png\" alt=\"The rate of data accumulation\" width=\"800\"/>\n",
    "\n",
    "All the Python calculations are performed at the driver.\n",
    "The driver also instructs the nodes to load portions of the datasets and perform computations.\n",
    "The results of the computations get sent to the driver node.\n",
    "The commands are executed only when the result needs to be materialized (displayed/stored/etc).\n",
    "\n",
    "For more background information, see: https://spark.apache.org/docs/latest/cluster-overview.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark application\n",
    "PySpark is a wrapper around Spark commands written in Scala.\n",
    "\n",
    "Please download this NY Taxi data from here https://www.kaggle.com/datasets/neilclack/nyc-taxi-trip-data-google-public-data to practice PySpark skills through the exercises below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"myAppName\")\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: reading the trips dataset from taxi_trip_data.csv file in data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_trips = spark.read.csv(\"data/taxi_trip_data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: read the geo zones dataset from the taxi_zone_geo.csv file in the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_geo = spark.read.csv(\"data/taxi_zone_geo.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Display rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: show a sample of 5 records from sdf_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_trips.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: show a sample of 3 records from sdf_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_geo.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: show the top 3 most used payment types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sdf_trips\n",
    "    .groupBy('payment_type')\n",
    "    .count()\n",
    "    .sort(F.desc('count'))\n",
    ").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: show the top 3 borough names with the most zone ids\n",
    "\n",
    "Extra credit for displaying the percentage of total zone ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_row_count = sdf_geo.count()\n",
    "\n",
    "(\n",
    "    sdf_geo\n",
    "    .groupBy('borough')\n",
    "    .count()\n",
    "    .sort(F.desc('count'))\n",
    "    .withColumn('percentage', F.format_string('%2.2f%%', 100*F.col('count')/geo_row_count))\n",
    "    .drop('all_counts')\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: count of the rides with more than 3 passengers in the year May of 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_full_car_rides = (\n",
    "    sdf_trips\n",
    "    .filter(F.date_format(F.col('pickup_datetime'), \"MMyyyy\") == \"052018\")\n",
    "    .filter(F.col('passenger_count') > 4)\n",
    ").count()\n",
    "\n",
    "print(f\"{n_full_car_rides:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: find the cost of the most expensive ride that ended between 5pm and 6pm made paid with the payment type 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_evening_ride = (\n",
    "    sdf_trips\n",
    "    # .withColumn(\"hours\", F.date_format(F.col('pickup_datetime'), \"HH\"))\n",
    "    .filter(F.date_format(F.col('dropoff_datetime'), \"HH\") == \"17\")\n",
    "    .filter(F.col('payment_type') == 1)\n",
    "    .agg(F.max(\"fare_amount\"))\n",
    ").collect()[0][0]\n",
    "\n",
    "print(f\"${cost_evening_ride:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Deduplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: keep only one trip for each unique combination of passenger_count and rate_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sdf_trips\n",
    "    .dropDuplicates(subset=[\"passenger_count\", \"rate_code\"])\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: keep only one zone id for each zone name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sdf_geo\n",
    "    .dropDuplicates(subset=[\"zone_name\"])\n",
    "    .groupBy('zone_name')\n",
    "    .count()\n",
    "    .sort(F.desc('count'))\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Change conditionally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: reduce the `tolls_amount` by half for the trips that started and ended in the different zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(\n",
    "    sdf_trips\n",
    "    .withColumn(\n",
    "        \"new_toll\",\n",
    "        F.when(F.col(\"pickup_location_id\") != F.col(\"dropoff_location_id\"), F.col(\"tolls_amount\") * 0.5)\n",
    "        .otherwise(F.col(\"tolls_amount\"))\n",
    "    )\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: set the negative fare_amount to 0 for the negative fares and cap the positive ones by $10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sdf_trips\n",
    "    .withColumn(\n",
    "        \"fare_amount\",\n",
    "        F.when(F.col(\"fare_amount\") < 0, F.lit(0))\n",
    "        .when(F.col(\"fare_amount\") > 10, F.lit(10))\n",
    "        .otherwise(F.col(\"fare_amount\"))\n",
    "    )\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: how many single passengers were picked up in Bronx borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_single_bronx_rides = (\n",
    "    sdf_trips\n",
    "    .filter(F.col(\"passenger_count\") == 1)\n",
    "    .join(\n",
    "        sdf_geo\n",
    "        .filter(F.col(\"borough\") == \"Bronx\"),\n",
    "        sdf_trips.pickup_location_id == sdf_geo.zone_id,\n",
    "        how=\"inner\",\n",
    "    )\n",
    ").count()\n",
    "\n",
    "print(f\"{n_single_bronx_rides:,} single Bronx rides\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: What is the longest ride to JFK Airport zone for less than $20?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_cheap_trip = (\n",
    "    sdf_trips\n",
    "    .filter(F.col(\"passenger_count\") == 1)\n",
    "    .join(\n",
    "        sdf_geo\n",
    "        .filter(F.col(\"zone_name\") == \"JFK Airport\"),\n",
    "        sdf_trips.dropoff_location_id == sdf_geo.zone_id,\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    .filter(F.col(\"fare_amount\") < 20)\n",
    "    .sort(F.desc(\"trip_distance\"))\n",
    "    .limit(1)\n",
    ").collect()[0][\"trip_distance\"]\n",
    "\n",
    "print(f\"Price for the longest cheap trip: ${longest_cheap_trip:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. User defined functions\n",
    "When you think you ran out of Pyspark native options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: round up the tip amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "def spark_ceil(x):\n",
    "    return ceil(x)\n",
    "\n",
    "spark_ceil_udf = F.udf(spark_ceil, T.IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sdf_trips\n",
    "    .withColumn(\"tip_amount_rounded_up\", spark_ceil_udf(F.col(\"tip_amount\")))\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: calculate cosine of the tolls amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import cos\n",
    "\n",
    "def spark_cos(x):\n",
    "    return cos(x)\n",
    "\n",
    "spark_cos_udf = F.udf(spark_cos, T.FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sdf_trips\n",
    "    .withColumn(\"cos_tolls_amount\", spark_cos_udf(F.col(\"tolls_amount\")))\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transformations within slices/windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: keep the 3 latest rides from each pickup zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sdf_trips\n",
    "    .withColumn(\n",
    "        \"row_number\",\n",
    "        F.row_number()\n",
    "        .over(\n",
    "            Window\n",
    "            .partitionBy(\"pickup_location_id\")\n",
    "            .orderBy(F.desc(\"pickup_datetime\"))\n",
    "        )\n",
    "    )\n",
    "    .filter(F.col(\"row_number\") < 4)\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: calculate a cumulative sum of mta_tax starting from the earliest for each payment type\n",
    "\n",
    "Extra credit: how many trips did it take to accumulate $5 in mta_tax for each payment type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO_ACCUMULATE = 5\n",
    "\n",
    "(\n",
    "    sdf_trips\n",
    "    .withColumn(\n",
    "        \"cumulative_mta_tax\",\n",
    "        F.sum(\"mta_tax\")\n",
    "        .over(\n",
    "            Window\n",
    "            .partitionBy(\"payment_type\")\n",
    "            .orderBy(F.col(\"pickup_datetime\"))\n",
    "            .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "        )\n",
    "    )\n",
    "    .filter(F.col(\"cumulative_mta_tax\") <= TO_ACCUMULATE)\n",
    "    .groupBy(\"payment_type\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"trips_count\"),\n",
    "        F.max(\"cumulative_mta_tax\").alias(\"max_cum\")\n",
    "    )\n",
    "    .select(\n",
    "        \"payment_type\",\n",
    "        F.when(F.col(\"max_cum\") >= TO_ACCUMULATE, F.col(\"trips_count\"))\n",
    "        .otherwise(F.lit(None))\n",
    "        .alias(\"trips_count\")\n",
    "    )\n",
    ").show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Structures and Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: put pickup and dropoff locations to the zone_ids structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sdf_trips\n",
    "    .withColumn(\n",
    "        \"zone_ids\",\n",
    "        F.struct('pickup_location_id','dropoff_location_id')\n",
    "    )\n",
    ").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: create a column that would hold a structure called payments that would include the total and substructure with all the contributions to this payment\n",
    "\n",
    "**Task 2**: create a dataframe `sdf_structured` with the column above while the contributing columns are dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_structured = (\n",
    "    sdf_trips\n",
    "    .withColumn(\n",
    "        \"contributions\",\n",
    "        F.struct(*sdf_trips.columns[8:12])\n",
    "    )\n",
    "    .withColumn(\n",
    "        'payments',\n",
    "        F.struct(\"contributions\", \"total_amount\")\n",
    "    )\n",
    "    .drop(*sdf_trips.columns[8:13])\n",
    "    .drop(\"contributions\")\n",
    ")\n",
    "\n",
    "sdf_structured.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: collect an array of trip distances for each `passenger_count`, `rate_code`, and `payment_type` combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sdf_trips\n",
    "    .groupBy(\n",
    "        \"passenger_count\",\n",
    "        \"rate_code\",\n",
    "        \"payment_type\",\n",
    "    )\n",
    "    .agg(\n",
    "        F.collect_list('trip_distance').alias('distances')\n",
    "    )\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: create a column that holds the 5 latest payments structures from `sdf_structured` for each pickup location and store the resulting data frame as `sdf_structured_arrayed`\n",
    "\n",
    "**Extra Credit**: extract the 3rd tip amount from the array for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_structured_arrayed = (\n",
    "    sdf_structured\n",
    "    .withColumn(\n",
    "        \"row_number\",\n",
    "        F.row_number()\n",
    "        .over(\n",
    "            Window\n",
    "            .partitionBy(\"pickup_location_id\")\n",
    "            .orderBy(F.desc(\"pickup_datetime\"))\n",
    "        )\n",
    "    )\n",
    "    .filter(F.col(\"row_number\") < 6)\n",
    "    .groupBy(\"pickup_location_id\")\n",
    "    .agg(\n",
    "        F.collect_list('payments').alias('payments')\n",
    "    )\n",
    "    .withColumnRenamed(\"pickup_location_id\", \"zone_id\")\n",
    "    .join(\n",
    "        sdf_geo,\n",
    "        on=\"zone_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    ")\n",
    "sdf_structured_arrayed.printSchema()\n",
    "\n",
    "(\n",
    "    sdf_structured_arrayed\n",
    "    .select(\n",
    "        \"zone_name\",\n",
    "        \"payments\",\n",
    "        (F.col(\"payments.contributions.tip_amount\")[2]).alias('the_trird_tip')\n",
    "    )\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Extract data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Extract total amount from `sdf_structured` into individual rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sdf_structured_arrayed\n",
    "    .withColumn('payments', F.explode('payments'))\n",
    "    .select(\"*\", \"payments.total_amount\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Extract contributions from `sdf_structured` into individual rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sdf_structured_arrayed\n",
    "    .withColumn('payments', F.explode('payments'))\n",
    "    .select(\"*\", \"payments.contributions.*\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Format time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: keep only the rides that went over at least one night and change pickup and dropoff times to the format like `April 27, 1967`\n",
    "\n",
    "Make sure to exclude the ones traveling back in time (dropped off before picked up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sdf_trips\n",
    "    .withColumn(\n",
    "        \"date_diff\",\n",
    "        F.datediff('dropoff_datetime', 'pickup_datetime')\n",
    "    )\n",
    "    .filter(F.col(\"date_diff\") > 0)\n",
    "    .withColumn('pickup_date', F.date_format(F.col(\"pickup_datetime\"), 'MMMM d, yyy'))\n",
    "    .withColumn('dropoff_date', F.date_format(F.col(\"dropoff_datetime\"), 'MMMM d, yyy'))\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: keep only the weekend rides (started on either Saturday or Sunday) and specify which day it was in the corresponding column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sdf_trips\n",
    "    .withColumn(\n",
    "        \"day_of_week\",\n",
    "        F.dayofweek('pickup_datetime')\n",
    "    )\n",
    "    .filter(F.col(\"day_of_week\").isin([1, 7]))\n",
    "    .withColumn(\n",
    "        \"day_of_week\",\n",
    "        F.when(F.col(\"day_of_week\") == 1, F.lit(\"Sunday\"))\n",
    "        .otherwise(F.lit(\"Saturday\"))\n",
    "    )\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Unix time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: keep only the rides that ended on the day corresponding to the following unix timestamp `1521552311`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sdf_trips\n",
    "    .filter(F.to_date(\"dropoff_datetime\") == F.to_date(F.from_unixtime(F.lit(\"1521552311\"))))\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: find the date corresponding twice the unix pickup date for each ride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sdf_trips\n",
    "    .withColumn(\n",
    "        \"ts_doubled\",\n",
    "        2*F.unix_timestamp('pickup_datetime')\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"doubled_date\",\n",
    "        F.to_date(F.from_unixtime(F.col(\"ts_doubled\")))\n",
    "    )\n",
    "    .sample(0.01)\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Drying the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: rewrite the example in part 3 with window defined in a separate variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_pickup = Window.partitionBy(\"pickup_location_id\").orderBy(F.desc(\"pickup_datetime\"))\n",
    "\n",
    "(\n",
    "    sdf_trips\n",
    "    .withColumn(\"row_number\", F.row_number().over(window_pickup))\n",
    "    .filter(F.col(\"row_number\") < 4)\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: rewrite the task from part 3 with window defined in a separate variable\n",
    "\n",
    "**Extra credit**: rewrite the filtering condition as a separate variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO_ACCUMULATE = 5\n",
    "window_payment_type = Window.partitionBy(\"payment_type\").orderBy(F.col(\"pickup_datetime\")).rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "limit_cum_mta = F.col(\"cumulative_mta_tax\") <= TO_ACCUMULATE\n",
    "\n",
    "(\n",
    "    sdf_trips\n",
    "    .withColumn(\n",
    "        \"cumulative_mta_tax\",\n",
    "        F.sum(\"mta_tax\")\n",
    "        .over(window_payment_type)\n",
    "    )\n",
    "    .filter(limit_cum_mta)\n",
    "    .groupBy(\"payment_type\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"trips_count\"),\n",
    "        F.max(\"cumulative_mta_tax\").alias(\"max_cum\")\n",
    "    )\n",
    "    .select(\n",
    "        \"payment_type\",\n",
    "        F.when(F.col(\"max_cum\") >= TO_ACCUMULATE, F.col(\"trips_count\"))\n",
    "        .otherwise(F.lit(None))\n",
    "        .alias(\"trips_count\")\n",
    "    )\n",
    ").show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Parameterization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: multiply each of the monetary columns by the number of symbols in the column name and add this value to the column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sdf_trips\n",
    "    .select(\n",
    "        *sdf_trips.columns[:8],\n",
    "        *[(F.col(c)*len(c)).alias(f\"{c} * {len(c)}\") for c in sdf_trips.columns[8:-2]],\n",
    "        *sdf_trips.columns[8:-2],\n",
    "        *sdf_trips.columns[-2:],\n",
    "    )\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: rewrite the example from 5.1 by creating a function that takes the date time column name and returns the formatted output\n",
    "\n",
    "**Extra credit**: make the function work in a loop by providing a list of columns to transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_date(col):\n",
    "    return F.date_format(F.col(col), 'MMMM d, yyy')\n",
    "\n",
    "(\n",
    "    sdf_trips\n",
    "    .withColumn(\n",
    "        \"date_diff\",\n",
    "        F.datediff('dropoff_datetime', 'pickup_datetime')\n",
    "    )\n",
    "    .filter(F.col(\"date_diff\") > 0)\n",
    "    .select(\n",
    "        \"*\",\n",
    "        *[transform_date(col).alias(col[:-4])\n",
    "          for col in sdf_trips.columns\n",
    "          if \"datetime\" in col]\n",
    "    )\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: calculate the number of combinations of rides such that the drop off location for one is the pick up location for another\n",
    "\n",
    "**Hint**: stop the execution if spent more than 5 minutes waiting and proceed to the following task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_pairs = (\n",
    "    sdf_trips\n",
    "    .select(\n",
    "        F.col(\"pickup_location_id\").alias(\"pickup_location_id_a\"),\n",
    "        F.col(\"dropoff_location_id\").alias(\"dropoff_location_id_a\"),\n",
    "        F.col(\"dropoff_location_id\").alias(\"to_join\"),\n",
    "    )\n",
    "    .join(\n",
    "        sdf_trips\n",
    "        .select(\n",
    "            F.col(\"pickup_location_id\").alias(\"pickup_location_id_b\"),\n",
    "            F.col(\"dropoff_location_id\").alias(\"dropoff_location_id_b\"),\n",
    "            F.col(\"pickup_location_id\").alias(\"to_join\"),\n",
    "        ),\n",
    "        on=\"to_join\",\n",
    "        how='inner',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the combinations\n",
    "# print(f\"{sdf_sampled_pairs.count()=:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: prototype the code in example for the 0.1% sample of the rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_trips_sample = sdf_trips.sample(0.001)\n",
    "\n",
    "sdf_sampled_pairs = (\n",
    "    sdf_trips_sample\n",
    "    .select(\n",
    "        F.col(\"pickup_location_id\").alias(\"pickup_location_id_a\"),\n",
    "        F.col(\"dropoff_location_id\").alias(\"dropoff_location_id_a\"),\n",
    "        F.col(\"dropoff_location_id\").alias(\"to_join\"),\n",
    "    )\n",
    "    .join(\n",
    "        sdf_trips_sample\n",
    "        .select(\n",
    "            F.col(\"pickup_location_id\").alias(\"pickup_location_id_b\"),\n",
    "            F.col(\"dropoff_location_id\").alias(\"dropoff_location_id_b\"),\n",
    "            F.col(\"pickup_location_id\").alias(\"to_join\"),\n",
    "        ),\n",
    "        on=\"to_join\",\n",
    "        how='inner',\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"{sdf_sampled_pairs.count()=:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Saving intermediate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: For the task in 7.1: save the first 0.1% of rides before joining it to the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_trips_sample = sdf_trips.sample(0.001)\n",
    "\n",
    "# (\n",
    "#     sdf_trips_sample\n",
    "#     .write.mode(\"overwrite\")\n",
    "#     .option(\"overwriteSchema\", \"True\")\n",
    "#     .format(\"parquet\")\n",
    "#     .saveAsTable(\"sdf_trips_sample\")\n",
    "# )\n",
    "\n",
    "# To be continued\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: process at least 10 batches of the 0.1% samples to get the corresponding scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Duck typing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: make `.prc()` applied to a dataframe print row count of that dataframe and test it on `sdf_trips` dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "def _prc(self):\n",
    "    print(f\"{self.count():,}\")\n",
    "\n",
    "DataFrame.prc = _prc\n",
    "\n",
    "sdf_trips.prc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: make `.pvc(col)` applied to a dataframe return another dataframe with row count for each unique entry in the column `col` and test it on `sdf_geo` dataset column `borough`\n",
    "\n",
    "**Extra credit** the dataframe should also contain a column for the percentage of each unique entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pvc(self, col):\n",
    "    return (\n",
    "        self\n",
    "        .withColumn('total_count', F.count(\"*\").over(Window.partitionBy(F.lit(1))))\n",
    "        .groupBy(col)\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"count\"),\n",
    "            F.format_string('%.1f%%', 100*F.count(\"*\")/F.max('total_count')).alias(\"percentage\"),\n",
    "        )\n",
    "        .sort(F.desc(\"count\"))\n",
    "    )\n",
    "\n",
    "DataFrame.pvc = _pvc\n",
    "\n",
    "sdf_geo.pvc('borough').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_training",
   "language": "python",
   "name": "pyspark_training"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
